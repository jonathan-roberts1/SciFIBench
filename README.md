# SciFIBench

[**SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation**](https://github.com/jonathan-roberts1/SciFIBench/blob/main/SciFIBench.pdf)

Jonathan Roberts, Kai Han, Neil Houlsby, Samuel Albanie

[[Project Page - coming soon!]()] [[Paper](https://github.com/jonathan-roberts1/SciFIBench/blob/main/SciFIBench.pdf)] [[Data](https://huggingface.co/datasets/jonathan-roberts1/SciFIBench)]

## Key insights:
- We use adversarial filtering and human verification to curate a challenging, high-quality 1000-question scientific figure interpretation benchmark.
- We evaluate 30 LMM, VLM and human baselines on our SciFIBench.
- GPT-4o and Gemini-Pro 1.5 are the best-performing models, outperforming some humans.
- The mean human score still outperforms all evaluated models.
- GPT-4o is significantly better than GPT-4V.
- Leveraging a strong LLM provides robust and accurate automatic evaluation.
- Varying levels of faithfulness in question answering are shown by the LMMs evaluated.

## Curation
![](images/curation.png '')

## Main results
![](images/results.png '')

## Alignment
![](images/alignment.png '')
![](images/alignment_results.png '')




